{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab3` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n",
    "## Deadlines ([AoE](https://en.wikipedia.org/wiki/Anywhere_on_Earth))\n",
    "\n",
    "* Sunday, December 4th for Task3.1 and Task3.2\n",
    "* Sunday, December 11th for Task3.3 and Task3.4\n",
    "* Sunday, December 18th for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "from functools import *\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import *\n",
    "from operator import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self._rows))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__hash__() == hash(other)\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(strategyA: Callable, strategyB: Callable, num_matches = 1, nim_size = 3, k = None) -> float:\n",
    "    players = (strategyA, strategyB)\n",
    "    won = 0\n",
    "\n",
    "    for _ in range(num_matches):\n",
    "        nim = Nim(nim_size, k)\n",
    "        player = 1\n",
    "        while nim:\n",
    "            ply = players[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return won / num_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random non empty row and remove a random number of objects smaller than min(k, row_objects)\n",
    "\n",
    "def random_strategy(state: Nim):\n",
    "    r = random.choice([idx for idx, r in enumerate(state.rows) if r > 0])\n",
    "    num_objects = random.randint(1, min(state.rows[r], state.k) if state.k != None else state.rows[r])\n",
    "\n",
    "    return (r, num_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Fixed-Rule Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among all possible moves, simply do this:\n",
    "# - if there is a winning move, choose it\n",
    "# - if there is not a winning move but the move puts the opponent in a winning situation, discard it\n",
    "# - otherwise choose the first move possible, even if not optimal (obliged to do a move)\n",
    "\n",
    "def fixed_strategy(state: Nim):\n",
    "    possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "    move = None\n",
    "    firstMove = None\n",
    "\n",
    "    for m in possible_moves:\n",
    "        if firstMove == None:\n",
    "            firstMove = Nimply(m[0], m[1])\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        if not tmp:\n",
    "            return Nimply(m[0], m[1])\n",
    "        else:\n",
    "            active_rows = len([r for r in state.rows if r > 0])\n",
    "            eliminable_rows = len([r for r in state.rows if r > 0 and (state.k == None or r < state.k)])\n",
    "            if active_rows == eliminable_rows and eliminable_rows == 1:\n",
    "                continue\n",
    "            elif move == None:\n",
    "                move = Nimply(m[0], m[1])\n",
    "\n",
    "    return move if move != None else firstMove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Evolved Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from statistics import *\n",
    "\n",
    "GENOME_LENGTH = 11 # NB: GENOME_LENGTH must be of the form 4*n + 3 with n > 0, if n > 1 it may converge to the xor solution\n",
    "GAME_PARS = list((a, b) for a, b in product(range(2, 10, 3), list(range(1, 10, 3)) + [None]) if b == None or b < a)\n",
    "GAMES = len(GAME_PARS)\n",
    "\n",
    "def decode_genome(genome):\n",
    "    assert (GENOME_LENGTH - 3) % 4 == 0 and GENOME_LENGTH > 0, 'GENOME_LENGTH must be of the form 4*n + 3 with n > 0'\n",
    "    out = \"\"\n",
    "\n",
    "    for op_start in range(0, GENOME_LENGTH, 4):\n",
    "        tmpA = 'a' if genome[op_start] < 0.5 else '!a'\n",
    "        tmpB = 'b' if genome[op_start + 2] < 0.5 else '!b'\n",
    "        internal_op = '&' if genome[op_start + 1] < 0.5 else '|'\n",
    "        op = ('&' if genome[op_start + 3] < 0.5 else '|') if op_start + 3 < GENOME_LENGTH else ''\n",
    "        out += f'({tmpA} {internal_op} {tmpB}) {op} '\n",
    "\n",
    "    return out[:-2]\n",
    "\n",
    "def evolvable_strategy(genome):\n",
    "    assert (GENOME_LENGTH - 3) % 4 == 0 and GENOME_LENGTH > 0, 'GENOME_LENGTH must be of the form 4*n + 3 with n > 0'\n",
    "\n",
    "    def genetic_op(a, b):\n",
    "        result = 0\n",
    "        op = lambda _, b: b\n",
    "\n",
    "        for op_start in range(0, GENOME_LENGTH, 4):\n",
    "            tmpA = a if genome[op_start] < 0.5 else ~a\n",
    "            tmpB = b if genome[op_start + 2] < 0.5 else ~b\n",
    "            internal_op = and_ if genome[op_start + 1] < 0.5 else or_\n",
    "            result = op(result, internal_op(tmpA, tmpB))\n",
    "            op = (and_ if genome[op_start + 3] < 0.5 else or_) if op_start + 3 < GENOME_LENGTH else None\n",
    "\n",
    "        return result\n",
    "\n",
    "    def strategy(state: Nim):\n",
    "            possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k]\n",
    "            best = None\n",
    "\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                val = reduce(genetic_op, tmp.rows)\n",
    "\n",
    "                if best == None or best[1] > val:\n",
    "                    best = (m, val)\n",
    "                             \n",
    "            return best[0]\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "def mutation(genome):\n",
    "    point = random.randint(0, len(genome) - 1)\n",
    "    return genome[:point] + [1 - genome[point]] + genome[point + 1:]\n",
    "\n",
    "def crossover(genomeA, genomeB):\n",
    "    p = random.random()\n",
    "    return [x if p < 0.5 else y for x, y in zip(genomeA, genomeB)]\n",
    "\n",
    "def tournament(population, tournament_size):\n",
    "    return max(random.choices(population, k=tournament_size), key=lambda i: i.fitness)\n",
    "\n",
    "def fitness(genome):\n",
    "    win_optimal = 0.0\n",
    "    win_random = 0.0\n",
    "\n",
    "    for nim_size, k in GAME_PARS:\n",
    "        home = evaluate(evolvable_strategy(genome), random_strategy, nim_size=nim_size, k=k) \n",
    "        away = 1 - evaluate(random_strategy, evolvable_strategy(genome), nim_size=nim_size, k=k)\n",
    "        win_random += home + away\n",
    "        home = evaluate(evolvable_strategy(genome), optimal_strategy, nim_size=nim_size, k=k) \n",
    "        away = 1 - evaluate(optimal_strategy, evolvable_strategy(genome), nim_size=nim_size, k=k)\n",
    "        win_optimal += home + away\n",
    "\n",
    "    return (win_optimal/(2*GAMES), win_random/(2*GAMES))\n",
    "        \n",
    "def genetic_algorithm():\n",
    "    Individual = namedtuple('Individual', ('genome', 'fitness'))\n",
    "\n",
    "    NUM_GENS = 100    \n",
    "    POPULATION_SIZE = 10\n",
    "    OFFSPRING_SIZE = 20\n",
    "    TOURNAMENT_SIZE = 2\n",
    "    USELESS_GENS = 0\n",
    "    STEADY_STATE_LIMIT = 5\n",
    "\n",
    "    population = [Individual(i, fitness(i)) for i in ([round(random.random(), 2) for _ in range(GENOME_LENGTH)] for _ in range(POPULATION_SIZE))]\n",
    "    best = None\n",
    "    \n",
    "    for g in range(NUM_GENS):\n",
    "        offspring = list()\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "            if random.random() < 0.3:\n",
    "                p = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                o = mutation(p.genome)\n",
    "            else:\n",
    "                p1 = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                p2 = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                o = crossover(p1.genome, p2.genome)\n",
    "            f = fitness(o)\n",
    "            offspring.append(Individual(o, f))\n",
    "        population += offspring\n",
    "        population = sorted(population, key=lambda i: i.fitness, reverse=True)[:POPULATION_SIZE]\n",
    "        newBest = max(population, key=lambda i: i.fitness)\n",
    "\n",
    "        if best != None and newBest <= best:\n",
    "            logging.info(f'Gen {g+1} skipped because useless')\n",
    "            USELESS_GENS += 1\n",
    "        else:\n",
    "            logging.info(f'Gen {g+1}, found new best individual: {decode_genome(newBest.genome)} with fitness = {newBest.fitness}')\n",
    "            best = newBest\n",
    "            USELESS_GENS = 0\n",
    "        \n",
    "        if USELESS_GENS == STEADY_STATE_LIMIT:\n",
    "            logging.info(f'Gen {g+1}, no improvements after {USELESS_GENS} gens, terminating...')\n",
    "            break\n",
    "\n",
    "    logging.info(f'Best individual: {decode_genome(best.genome)} with genome {best.genome} fitness = {best.fitness}')\n",
    "\n",
    "    return evolvable_strategy(best.genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Min-Max Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_strategy_no_pruning(state: Nim):\n",
    "    def minmax(state: Nim, current_player, level = 0):\n",
    "        possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        current_func = min if current_player == 1 else max\n",
    "        tabs = '\\t' * level\n",
    "        if not state:\n",
    "            #logging.debug(f'{tabs} Level: {level}, Winner: {1 if level % 2 == 1 else -1}')\n",
    "            return None, current_player\n",
    "\n",
    "        evaluations = list()\n",
    "        for m in possible_moves:\n",
    "            tmp = deepcopy(state)\n",
    "            tmp.nimming(m)\n",
    "            #logging.debug(f\"{tabs} Level: {level}, Move: {m}, State: {tmp}, Player: {-current_player}\")\n",
    "            _, val = minmax(tmp, -current_player, level + 1)\n",
    "            evaluations.append((m, val))\n",
    "            \n",
    "        #logging.debug(f\"{tabs} Level: {level}, Evals: {evaluations}, Player: {-current_player}\")\n",
    "        return current_func(evaluations, key=lambda k: k[1])\n",
    "      \n",
    "\n",
    "    move, _ = minmax(state, 1)\n",
    "\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(max_depth=math.inf, print_stats = False):\n",
    "    max_depth_reached = 0\n",
    "    cache = {}\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    def minmax_with_pruning(state: Nim, alpha, beta, current_player, depth=0):\n",
    "        nonlocal max_depth_reached, cache, hits, misses\n",
    "        max_depth_reached = max(max_depth_reached, depth)\n",
    "                \n",
    "        if not state or depth >= max_depth:\n",
    "            return current_player, None # i.e. the loser\n",
    "        \n",
    "        possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        value = current_player * math.inf, None\n",
    "\n",
    "        if current_player == 1: # my turn, minimize\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                if (tmp, -current_player) not in cache:\n",
    "                    val, _ = minmax_with_pruning(tmp, alpha, beta, -current_player, depth + 1)\n",
    "                    cache.update({(tmp, -current_player): (val, None)}) \n",
    "                    misses += 1\n",
    "                else:\n",
    "                    val, _ = cache[(tmp, -current_player)]\n",
    "                    hits += 1\n",
    "\n",
    "                #logging.info(f'Move: {(val, m)} on {state} by player {current_player} at depth {depth}')\n",
    "                \n",
    "                value = min(value, (val, m))\n",
    "\n",
    "                if value <= alpha:\n",
    "                    break\n",
    "\n",
    "                beta = min(beta, value)   \n",
    "            return value\n",
    "        else: # its turn, maximize\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                if (tmp, -current_player) not in cache:\n",
    "                    val, _ = minmax_with_pruning(tmp, alpha, beta, -current_player, depth + 1)\n",
    "                    cache.update({(tmp, -current_player): (val, None)}) \n",
    "                    misses += 1\n",
    "                else:\n",
    "                    val, _ = cache[(tmp, -current_player)]\n",
    "                    hits += 1\n",
    "                \n",
    "                value = max(value, (val, m))\n",
    "                \n",
    "                if value >= beta:\n",
    "                    break\n",
    "\n",
    "                alpha = max(alpha, value)    \n",
    "            return value\n",
    "\n",
    "    def minmax_strategy_with_pruning(state: Nim):\n",
    "        _, move = minmax_with_pruning(state, (-math.inf, None), (math.inf, None), 1)\n",
    "        nonlocal max_depth_reached, hits, misses\n",
    "        \n",
    "        if print_stats:        \n",
    "            logging.debug(f'Max depth reached: {max_depth_reached}, Cache hit ration: {round(hits/(hits+misses), 3)*100}% ({hits}/{hits+misses})')\n",
    "\n",
    "        return move\n",
    "\n",
    "    return minmax_strategy_with_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: Reinforcement Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learning(initial_state: Nim, training_epochs = 100, randomness = 0.3, learning_rate = 0.15, max_depth = math.inf, plot_stats = False):\n",
    "    def rl_strategy(state: Nim):\n",
    "        return choose_next_move(state, 0)\n",
    "\n",
    "    def choose_next_move(state: Nim, randomness):\n",
    "        allowed_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        maxQ = -10e15\n",
    "        next_move = None\n",
    "        randomN = random.random()\n",
    "        if randomN < randomness:\n",
    "            next_move = random.choice(list(allowed_moves))\n",
    "            new_state = deepcopy(state)\n",
    "            new_state.nimming(next_move)\n",
    "            Q.update({ new_state: random.random() }) if new_state not in Q else None\n",
    "        else:\n",
    "            for move in allowed_moves:\n",
    "                new_state = deepcopy(state)\n",
    "                new_state.nimming(move)\n",
    "                Q.update({ new_state: random.random() }) if new_state not in Q else None\n",
    "\n",
    "                if Q[new_state] >= maxQ:\n",
    "                    next_move = move\n",
    "                    maxQ = Q[new_state]\n",
    "\n",
    "        return next_move\n",
    "\n",
    "    def eval_state(state: Nim):\n",
    "        return 0 if state else 1\n",
    "\n",
    "    def learn(states_history, randomness, learning_rate):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(states_history):\n",
    "            Q[prev] += learning_rate * (target - Q[prev])\n",
    "            target += reward\n",
    "\n",
    "        states_history = []\n",
    "        randomness -= 10e-5  \n",
    "    \n",
    "    current_state = deepcopy(initial_state)\n",
    "    Q = {}\n",
    "    states_history = []\n",
    "    moveHistory = []\n",
    "    indices = []\n",
    "    depth = 0    \n",
    "\n",
    "    for i in range(training_epochs):\n",
    "        while current_state:\n",
    "            move = choose_next_move(deepcopy(current_state), randomness)\n",
    "            current_state.nimming(move)  \n",
    "            reward = eval_state(deepcopy(current_state))\n",
    "            depth += 1\n",
    "            states_history.append((deepcopy(current_state), reward))\n",
    "            if depth >= max_depth:\n",
    "                break\n",
    "        learn(states_history, randomness, learning_rate)  \n",
    "        if plot_stats and i % 50 == 0:\n",
    "            moveHistory.append(depth)\n",
    "            indices.append(i)\n",
    "        current_state = deepcopy(initial_state)\n",
    "        depth = 0\n",
    "\n",
    "    if plot_stats:\n",
    "        logging.getLogger().setLevel(logging.ERROR)\n",
    "        plt.semilogy(indices, moveHistory, \"b\")\n",
    "        logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "    return rl_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversimplified match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "status: Initial board  -> <1 3 5 7 9 11>\n",
      "status: After player 0 -> <0 3 5 7 9 11>\n",
      "status: After player 1 -> <0 0 5 7 9 11>\n",
      "status: After player 0 -> <0 0 4 7 9 11>\n",
      "status: After player 1 -> <0 0 4 6 9 11>\n",
      "status: After player 0 -> <0 0 3 6 9 11>\n",
      "status: After player 1 -> <0 0 3 1 9 11>\n",
      "status: After player 0 -> <0 0 3 1 8 11>\n",
      "status: After player 1 -> <0 0 2 1 8 11>\n",
      "status: After player 0 -> <0 0 2 0 8 11>\n",
      "status: After player 1 -> <0 0 2 0 8 10>\n",
      "status: After player 0 -> <0 0 2 0 7 10>\n",
      "status: After player 1 -> <0 0 2 0 7 5>\n",
      "status: After player 0 -> <0 0 1 0 7 5>\n",
      "status: After player 1 -> <0 0 1 0 4 5>\n",
      "status: After player 0 -> <0 0 0 0 4 5>\n",
      "status: After player 1 -> <0 0 0 0 4 4>\n",
      "status: After player 0 -> <0 0 0 0 3 4>\n",
      "status: After player 1 -> <0 0 0 0 3 3>\n",
      "status: After player 0 -> <0 0 0 0 2 3>\n",
      "status: After player 1 -> <0 0 0 0 2 2>\n",
      "status: After player 0 -> <0 0 0 0 1 2>\n",
      "status: After player 1 -> <0 0 0 0 1 1>\n",
      "status: After player 0 -> <0 0 0 0 0 1>\n",
      "status: After player 1 -> <0 0 0 0 0 0>\n",
      "status: Player 1 won!\n"
     ]
    }
   ],
   "source": [
    "nim = Nim(6, None)\n",
    "logging.debug(f\"status: Initial board  -> {nim}\")\n",
    "player = 0\n",
    "strategy = (minmax(), optimal_strategy)\n",
    "\n",
    "while nim:\n",
    "    ply = strategy[player](nim) \n",
    "    nim.nimming(ply)\n",
    "    logging.debug(f\"status: After player {player} -> {nim}\")\n",
    "    player = 1 - player\n",
    "winner = 1 - player\n",
    "logging.info(f\"status: Player {winner} won!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Game 0: Nim(2, 1)\n",
      "Game 1: Nim(2, None)\n",
      "Game 2: Nim(3, 1)\n",
      "Game 3: Nim(3, 2)\n",
      "Game 4: Nim(3, None)\n",
      "Game 5: Nim(4, 1)\n",
      "Game 6: Nim(4, 2)\n",
      "Game 7: Nim(4, 3)\n",
      "Game 8: Nim(4, None)\n",
      "Game 9: Nim(5, 1)\n",
      "Game 10: Nim(5, 2)\n",
      "Game 11: Nim(5, 3)\n",
      "Game 12: Nim(5, 4)\n",
      "Game 13: Nim(5, None)\n",
      "Game 14: Nim(6, 1)\n",
      "Game 15: Nim(6, 2)\n",
      "Game 16: Nim(6, 3)\n",
      "Game 17: Nim(6, 4)\n",
      "Game 18: Nim(6, 5)\n",
      "Game 19: Nim(6, None)\n",
      "Game 20: Nim(7, 1)\n",
      "Game 21: Nim(7, 2)\n",
      "Game 22: Nim(7, 3)\n",
      "Game 23: Nim(7, 4)\n",
      "Game 24: Nim(7, 5)\n",
      "Game 25: Nim(7, 6)\n",
      "Game 26: Nim(7, None)\n",
      "Game 27: Nim(8, 1)\n",
      "Game 28: Nim(8, 2)\n",
      "Game 29: Nim(8, 3)\n",
      "Game 30: Nim(8, 4)\n",
      "Game 31: Nim(8, 5)\n",
      "Game 32: Nim(8, 6)\n",
      "Game 33: Nim(8, 7)\n",
      "Game 34: Nim(8, None)\n",
      "Game 35: Nim(9, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [215], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m             scores[opponent_name][idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m evaluate(strategy, opponent, nim_size\u001b[39m=\u001b[39mnim_size, k\u001b[39m=\u001b[39mk) \n\u001b[0;32m     17\u001b[0m             strategy \u001b[39m=\u001b[39m strategy \u001b[39mif\u001b[39;00m strategy_name \u001b[39m!=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMinMax\u001b[39m\u001b[39m'\u001b[39m\u001b[39melse\u001b[39;00m minmax(\u001b[39m5\u001b[39m) \u001b[39m# Necessary to reset cache\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m             scores[opponent_name][idx] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m evaluate(opponent, strategy, nim_size\u001b[39m=\u001b[39;49mnim_size, k\u001b[39m=\u001b[39;49mk) \n\u001b[0;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m idx, strategy \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(strategies):\n\u001b[0;32m     21\u001b[0m     \u001b[39mfor\u001b[39;00m opponent_name, _ \u001b[39min\u001b[39;00m opponents:\n",
      "Cell \u001b[1;32mIn [75], line 9\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(strategyA, strategyB, num_matches, nim_size, k)\u001b[0m\n\u001b[0;32m      7\u001b[0m player \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39mwhile\u001b[39;00m nim:\n\u001b[1;32m----> 9\u001b[0m     ply \u001b[39m=\u001b[39m players[player](nim)\n\u001b[0;32m     10\u001b[0m     nim\u001b[39m.\u001b[39mnimming(ply)\n\u001b[0;32m     11\u001b[0m     player \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m player\n",
      "Cell \u001b[1;32mIn [214], line 40\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax_strategy\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminmax_strategy\u001b[39m(state: Nim):\n\u001b[1;32m---> 40\u001b[0m     _, move \u001b[39m=\u001b[39m minmax(state, (\u001b[39m-\u001b[39;49mmath\u001b[39m.\u001b[39;49minf, \u001b[39mNone\u001b[39;49;00m), (math\u001b[39m.\u001b[39;49minf, \u001b[39mNone\u001b[39;49;00m), \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m move\n",
      "Cell \u001b[1;32mIn [214], line 17\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax\u001b[1;34m(state, alpha, beta, current_player, depth)\u001b[0m\n\u001b[0;32m     14\u001b[0m tmp \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[0;32m     15\u001b[0m tmp\u001b[39m.\u001b[39mnimming(m)\n\u001b[1;32m---> 17\u001b[0m val, _ \u001b[39m=\u001b[39m minmax(tmp, alpha, beta, \u001b[39m-\u001b[39;49mcurrent_player, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(value, (val, m))\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m alpha:\n",
      "Cell \u001b[1;32mIn [214], line 30\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax\u001b[1;34m(state, alpha, beta, current_player, depth)\u001b[0m\n\u001b[0;32m     27\u001b[0m tmp \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[0;32m     28\u001b[0m tmp\u001b[39m.\u001b[39mnimming(m)\n\u001b[1;32m---> 30\u001b[0m val, _ \u001b[39m=\u001b[39m minmax(tmp, alpha, beta, \u001b[39m-\u001b[39;49mcurrent_player, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     31\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(value, (val, m))\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m beta:\n",
      "Cell \u001b[1;32mIn [214], line 17\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax\u001b[1;34m(state, alpha, beta, current_player, depth)\u001b[0m\n\u001b[0;32m     14\u001b[0m tmp \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[0;32m     15\u001b[0m tmp\u001b[39m.\u001b[39mnimming(m)\n\u001b[1;32m---> 17\u001b[0m val, _ \u001b[39m=\u001b[39m minmax(tmp, alpha, beta, \u001b[39m-\u001b[39;49mcurrent_player, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     18\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(value, (val, m))\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m alpha:\n",
      "Cell \u001b[1;32mIn [214], line 30\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax\u001b[1;34m(state, alpha, beta, current_player, depth)\u001b[0m\n\u001b[0;32m     27\u001b[0m tmp \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[0;32m     28\u001b[0m tmp\u001b[39m.\u001b[39mnimming(m)\n\u001b[1;32m---> 30\u001b[0m val, _ \u001b[39m=\u001b[39m minmax(tmp, alpha, beta, \u001b[39m-\u001b[39;49mcurrent_player, depth \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     31\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(value, (val, m))\n\u001b[0;32m     33\u001b[0m \u001b[39mif\u001b[39;00m value \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m beta:\n",
      "Cell \u001b[1;32mIn [214], line 14\u001b[0m, in \u001b[0;36mminmax.<locals>.minmax\u001b[1;34m(state, alpha, beta, current_player, depth)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mif\u001b[39;00m current_player \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m: \u001b[39m# my turn, minimize\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m possible_moves:\n\u001b[1;32m---> 14\u001b[0m         tmp \u001b[39m=\u001b[39m deepcopy(state)\n\u001b[0;32m     15\u001b[0m         tmp\u001b[39m.\u001b[39mnimming(m)\n\u001b[0;32m     17\u001b[0m         val, _ \u001b[39m=\u001b[39m minmax(tmp, alpha, beta, \u001b[39m-\u001b[39mcurrent_player, depth \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[0;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[0;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[1;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[0;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[0;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[0;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    204\u001b[0m append \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mappend\n\u001b[0;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x:\n\u001b[1;32m--> 206\u001b[0m     append(deepcopy(a, memo))\n\u001b[0;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mC:\\Python310\\lib\\copy.py:134\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeepcopy\u001b[39m(x, memo\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _nil\u001b[39m=\u001b[39m[]):\n\u001b[0;32m    129\u001b[0m     \u001b[39m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \n\u001b[0;32m    131\u001b[0m \u001b[39m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mif\u001b[39;00m memo \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m:\n\u001b[0;32m    135\u001b[0m         memo \u001b[39m=\u001b[39m {}\n\u001b[0;32m    137\u001b[0m     d \u001b[39m=\u001b[39m \u001b[39mid\u001b[39m(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "games = list((a, b) for a, b in product(range(2, 10), list(range(1, 10)) + [None]) if b == None or b < a)\n",
    "opponents = [('Optimal', optimal_strategy), ('Random', random_strategy)]\n",
    "strategies = [('MinMax', minmax(5))] # [('Fixed', fixed_strategy), ('Evolved', evolvable_strategy([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0])), ('MinMax', minmax(5)), ('RL', reinforcement_learning)]\n",
    "scores = {a[0]: [0.0 for _ in strategies] for a in opponents}\n",
    "\n",
    "for idx, pars in enumerate(games):\n",
    "    nim_size, k = pars\n",
    "    \n",
    "    logging.debug(f'Game {idx}: Nim({nim_size}, {k})')\n",
    "    \n",
    "    for idx, s in enumerate(strategies):\n",
    "        strategy_name, strategy = s\n",
    "        strategy = strategy if strategy_name !='RL'else strategy(Nim(nim_size, k), max_depth=20) # Necessary to train on new type of nim\n",
    "        for opponent_name, opponent in opponents:\n",
    "            strategy = strategy if strategy_name !='MinMax'else minmax(5) # Necessary to reset cache\n",
    "            scores[opponent_name][idx] += evaluate(strategy, opponent, nim_size=nim_size, k=k) \n",
    "            strategy = strategy if strategy_name !='MinMax'else minmax(5) # Necessary to reset cache\n",
    "            scores[opponent_name][idx] += 1 - evaluate(opponent, strategy, nim_size=nim_size, k=k) \n",
    "\n",
    "for idx, strategy in enumerate(strategies):\n",
    "    for opponent_name, _ in opponents:\n",
    "        logging.info(f'{strategy[0]} strategy win rate against {opponent_name} strategy was {round(scores[opponent_name][idx] * 100 / (2 * len(games)), 2)} % ({scores[opponent_name][idx]}/{2 * len(games)})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e660ce8e299eab6e1afd5ba1640493fbea599bc98ebfd90153bb9a99407a2701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
