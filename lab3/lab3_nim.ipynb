{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab3` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n",
    "## Deadlines ([AoE](https://en.wikipedia.org/wiki/Anywhere_on_Earth))\n",
    "\n",
    "* Sunday, December 4th for Task3.1 and Task3.2\n",
    "* Sunday, December 11th for Task3.3 and Task3.4\n",
    "* Sunday, December 18th for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "from functools import *\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import *\n",
    "from operator import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Nimply* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(tuple(self._rows))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.__hash__() == hash(other)\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(strategyA: Callable, strategyB: Callable, num_matches = 1, nim_size = 3, k = None) -> float:\n",
    "    players = (strategyA, strategyB)\n",
    "    won = 0\n",
    "\n",
    "    for _ in range(num_matches):\n",
    "        nim = Nim(nim_size, k)\n",
    "        player = 1\n",
    "        while nim:\n",
    "            ply = players[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return won / num_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random non empty row and remove a random number of objects smaller than min(k, row_objects)\n",
    "\n",
    "def random_strategy(state: Nim):\n",
    "    r = random.choice([idx for idx, r in enumerate(state.rows) if r > 0])\n",
    "    num_objects = random.randint(1, min(state.rows[r], state.k) if state.k != None else state.rows[r])\n",
    "\n",
    "    return (r, num_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Fixed-Rule Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among all possible moves, simply do this:\n",
    "# - if there is a winning move, choose it\n",
    "# - if there is not a winning move but the move puts the opponent in a winning situation, discard it\n",
    "# - otherwise choose the first move possible, even if not optimal (obliged to do a move)\n",
    "\n",
    "def fixed_strategy(state: Nim):\n",
    "    possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "    move = None\n",
    "    firstMove = None\n",
    "\n",
    "    for m in possible_moves:\n",
    "        if firstMove == None:\n",
    "            firstMove = Nimply(m[0], m[1])\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        if not tmp:\n",
    "            return Nimply(m[0], m[1])\n",
    "        else:\n",
    "            active_rows = len([r for r in state.rows if r > 0])\n",
    "            eliminable_rows = len([r for r in state.rows if r > 0 and (state.k == None or r < state.k)])\n",
    "            if active_rows == eliminable_rows and eliminable_rows == 1:\n",
    "                continue\n",
    "            elif move == None:\n",
    "                move = Nimply(m[0], m[1])\n",
    "\n",
    "    return move if move != None else firstMove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Evolved Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *\n",
    "from statistics import *\n",
    "\n",
    "GENOME_LENGTH = 11 # NB: GENOME_LENGTH must be of the form 4*n + 3 with n > 0, if n > 1 it may converge to the xor solution\n",
    "GAME_PARS = list((a, b) for a, b in product(range(2, 10, 3), list(range(1, 10, 3)) + [None]) if b == None or b < a)\n",
    "GAMES = len(GAME_PARS)\n",
    "\n",
    "def decode_genome(genome):\n",
    "    assert (GENOME_LENGTH - 3) % 4 == 0 and GENOME_LENGTH > 0, 'GENOME_LENGTH must be of the form 4*n + 3 with n > 0'\n",
    "    out = \"\"\n",
    "\n",
    "    for op_start in range(0, GENOME_LENGTH, 4):\n",
    "        tmpA = 'a' if genome[op_start] < 0.5 else '!a'\n",
    "        tmpB = 'b' if genome[op_start + 2] < 0.5 else '!b'\n",
    "        internal_op = '&' if genome[op_start + 1] < 0.5 else '|'\n",
    "        op = ('&' if genome[op_start + 3] < 0.5 else '|') if op_start + 3 < GENOME_LENGTH else ''\n",
    "        out += f'({tmpA} {internal_op} {tmpB}) {op} '\n",
    "\n",
    "    return out[:-2]\n",
    "\n",
    "def evolvable_strategy(genome):\n",
    "    assert (GENOME_LENGTH - 3) % 4 == 0 and GENOME_LENGTH > 0, 'GENOME_LENGTH must be of the form 4*n + 3 with n > 0'\n",
    "\n",
    "    def genetic_op(a, b):\n",
    "        result = 0\n",
    "        op = lambda _, b: b\n",
    "\n",
    "        for op_start in range(0, GENOME_LENGTH, 4):\n",
    "            tmpA = a if genome[op_start] < 0.5 else ~a\n",
    "            tmpB = b if genome[op_start + 2] < 0.5 else ~b\n",
    "            internal_op = and_ if genome[op_start + 1] < 0.5 else or_\n",
    "            result = op(result, internal_op(tmpA, tmpB))\n",
    "            op = (and_ if genome[op_start + 3] < 0.5 else or_) if op_start + 3 < GENOME_LENGTH else None\n",
    "\n",
    "        return result\n",
    "\n",
    "    def strategy(state: Nim):\n",
    "            possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k]\n",
    "            best = None\n",
    "\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                val = reduce(genetic_op, tmp.rows)\n",
    "\n",
    "                if best == None or best[1] > val:\n",
    "                    best = (m, val)\n",
    "                             \n",
    "            return best[0]\n",
    "    \n",
    "    return strategy\n",
    "\n",
    "def mutation(genome):\n",
    "    point = random.randint(0, len(genome) - 1)\n",
    "    return genome[:point] + [1 - genome[point]] + genome[point + 1:]\n",
    "\n",
    "def crossover(genomeA, genomeB):\n",
    "    p = random.random()\n",
    "    return [x if p < 0.5 else y for x, y in zip(genomeA, genomeB)]\n",
    "\n",
    "def tournament(population, tournament_size):\n",
    "    return max(random.choices(population, k=tournament_size), key=lambda i: i.fitness)\n",
    "\n",
    "def fitness(genome):\n",
    "    win_optimal = 0.0\n",
    "    win_random = 0.0\n",
    "\n",
    "    for nim_size, k in GAME_PARS:\n",
    "        home = evaluate(evolvable_strategy(genome), random_strategy, nim_size=nim_size, k=k) \n",
    "        away = 1 - evaluate(random_strategy, evolvable_strategy(genome), nim_size=nim_size, k=k)\n",
    "        win_random += home + away\n",
    "        home = evaluate(evolvable_strategy(genome), optimal_strategy, nim_size=nim_size, k=k) \n",
    "        away = 1 - evaluate(optimal_strategy, evolvable_strategy(genome), nim_size=nim_size, k=k)\n",
    "        win_optimal += home + away\n",
    "\n",
    "    return (win_optimal/(2*GAMES), win_random/(2*GAMES))\n",
    "        \n",
    "def genetic_algorithm():\n",
    "    Individual = namedtuple('Individual', ('genome', 'fitness'))\n",
    "\n",
    "    NUM_GENS = 100    \n",
    "    POPULATION_SIZE = 10\n",
    "    OFFSPRING_SIZE = 20\n",
    "    TOURNAMENT_SIZE = 2\n",
    "    USELESS_GENS = 0\n",
    "    STEADY_STATE_LIMIT = 5\n",
    "\n",
    "    population = [Individual(i, fitness(i)) for i in ([round(random.random(), 2) for _ in range(GENOME_LENGTH)] for _ in range(POPULATION_SIZE))]\n",
    "    best = None\n",
    "    \n",
    "    for g in range(NUM_GENS):\n",
    "        offspring = list()\n",
    "        for i in range(OFFSPRING_SIZE):\n",
    "            if random.random() < 0.3:\n",
    "                p = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                o = mutation(p.genome)\n",
    "            else:\n",
    "                p1 = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                p2 = tournament(population, tournament_size=TOURNAMENT_SIZE)\n",
    "                o = crossover(p1.genome, p2.genome)\n",
    "            f = fitness(o)\n",
    "            offspring.append(Individual(o, f))\n",
    "        population += offspring\n",
    "        population = sorted(population, key=lambda i: i.fitness, reverse=True)[:POPULATION_SIZE]\n",
    "        newBest = max(population, key=lambda i: i.fitness)\n",
    "\n",
    "        if best != None and newBest <= best:\n",
    "            logging.info(f'Gen {g+1} skipped because useless')\n",
    "            USELESS_GENS += 1\n",
    "        else:\n",
    "            logging.info(f'Gen {g+1}, found new best individual: {decode_genome(newBest.genome)} with fitness = {newBest.fitness}')\n",
    "            best = newBest\n",
    "            USELESS_GENS = 0\n",
    "        \n",
    "        if USELESS_GENS == STEADY_STATE_LIMIT:\n",
    "            logging.info(f'Gen {g+1}, no improvements after {USELESS_GENS} gens, terminating...')\n",
    "            break\n",
    "\n",
    "    logging.info(f'Best individual: {decode_genome(best.genome)} with genome {best.genome} fitness = {best.fitness}')\n",
    "\n",
    "    return evolvable_strategy(best.genome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Min-Max Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_strategy_no_pruning(state: Nim):\n",
    "    def minmax(state: Nim, current_player, level = 0):\n",
    "        possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        current_func = min if current_player == 1 else max\n",
    "\n",
    "        if not state:\n",
    "            return None, current_player\n",
    "\n",
    "        evaluations = list()\n",
    "        for m in possible_moves:\n",
    "            tmp = deepcopy(state)\n",
    "            tmp.nimming(m)\n",
    "            _, val = minmax(tmp, -current_player, level + 1)\n",
    "            evaluations.append((m, val))\n",
    "            \n",
    "        return current_func(evaluations, key=lambda k: k[1])\n",
    "      \n",
    "\n",
    "    move, _ = minmax(state, 1)\n",
    "\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(max_depth=math.inf, print_stats = False):\n",
    "    max_depth_reached = 0\n",
    "    cache = {}\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "    MAX_CACHE_LEN = 1e6\n",
    "\n",
    "    def minmax_with_pruning(state: Nim, alpha, beta, current_player, depth=0):\n",
    "        nonlocal max_depth_reached, cache, hits, misses\n",
    "        max_depth_reached = max(max_depth_reached, depth)\n",
    "                \n",
    "        if not state or depth >= max_depth:\n",
    "            return current_player, None # i.e. the loser\n",
    "        \n",
    "        possible_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        value = current_player * math.inf, None\n",
    "\n",
    "        if current_player == 1: # my turn, minimize\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                s =  (tmp, -current_player, alpha, beta)\n",
    "\n",
    "                if s not in cache or len(cache) >= MAX_CACHE_LEN:\n",
    "                    val, _ = minmax_with_pruning(tmp, alpha, beta, -current_player, depth + 1)\n",
    "                    cache.update({s: (val, None)}) \n",
    "                    misses += 1\n",
    "                else:\n",
    "                    val, _ = cache[s]\n",
    "                    hits += 1\n",
    "\n",
    "                #logging.info(f'Move: {(val, m)} on {state} by player {current_player} at depth {depth}')\n",
    "                \n",
    "                value = min(value, (val, m))\n",
    "\n",
    "                if value <= alpha:\n",
    "                    break\n",
    "\n",
    "                beta = min(beta, value)   \n",
    "            return value\n",
    "        else: # its turn, maximize\n",
    "            for m in possible_moves:\n",
    "                tmp = deepcopy(state)\n",
    "                tmp.nimming(m)\n",
    "                \n",
    "                s =  (tmp, -current_player, alpha, beta)\n",
    "\n",
    "                if s not in cache or len(cache) >= MAX_CACHE_LEN:\n",
    "                    val, _ = minmax_with_pruning(tmp, alpha, beta, -current_player, depth + 1)\n",
    "                    cache.update({s: (val, None)}) \n",
    "                    misses += 1\n",
    "                else:\n",
    "                    val, _ = cache[s]\n",
    "                    hits += 1\n",
    "                \n",
    "                value = max(value, (val, m))\n",
    "                \n",
    "                if value >= beta:\n",
    "                    break\n",
    "\n",
    "                alpha = max(alpha, value)    \n",
    "            return value\n",
    "\n",
    "    def minmax_strategy_with_pruning(state: Nim):\n",
    "        _, move = minmax_with_pruning(state, (-math.inf, None), (math.inf, None), 1)\n",
    "        nonlocal max_depth_reached, hits, misses\n",
    "        \n",
    "        if print_stats:        \n",
    "            logging.info(f'Max depth reached: {max_depth_reached}, Cache hit ratio: {round(hits/(hits+misses), 3)*100}% ({hits}/{hits+misses}), Cache entries : {len(cache)}')\n",
    "\n",
    "        return move\n",
    "\n",
    "    return minmax_strategy_with_pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4: Reinforcement Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learning(initial_state: Nim, training_epochs = 100, randomness = 0.3, learning_rate = 0.15, max_depth = math.inf, plot_stats = False):\n",
    "    def rl_strategy(state: Nim):\n",
    "        return choose_next_move(state, 0)\n",
    "\n",
    "    def opponent_move(state: Nim, opponentStrategy: Callable[[Nim], tuple[int, int]]):\n",
    "        next_move = opponentStrategy(state)\n",
    "        new_state = deepcopy(state)\n",
    "        new_state.nimming(next_move)\n",
    "        Q.update({ new_state: random.random() }) if new_state not in Q else None\n",
    "        return next_move\n",
    "\n",
    "    def choose_next_move(state: Nim, randomness):\n",
    "        allowed_moves = ((r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k)\n",
    "        maxQ = -10e15\n",
    "        next_move = None\n",
    "        randomN = random.random()\n",
    "        if randomN < randomness:\n",
    "            next_move = random.choice(list(allowed_moves))\n",
    "            new_state = deepcopy(state)\n",
    "            new_state.nimming(next_move)\n",
    "            Q.update({ new_state: random.random() }) if new_state not in Q else None\n",
    "        else:\n",
    "            for move in allowed_moves:\n",
    "                new_state = deepcopy(state)\n",
    "                new_state.nimming(move)\n",
    "                Q.update({ new_state: random.random() }) if new_state not in Q else None\n",
    "\n",
    "                if Q[new_state] >= maxQ:\n",
    "                    next_move = move\n",
    "                    maxQ = Q[new_state]\n",
    "\n",
    "        return next_move\n",
    "\n",
    "    def eval_state(state: Nim):\n",
    "        return 0 if state else 1\n",
    "\n",
    "    def learn(states_history, randomness, learning_rate):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(states_history):\n",
    "            Q[prev] += learning_rate * (target - Q[prev])\n",
    "            target += reward\n",
    "\n",
    "        states_history = []\n",
    "        randomness -= 10e-5  \n",
    "    \n",
    "    current_state = deepcopy(initial_state)\n",
    "    Q = {}\n",
    "    states_history = []\n",
    "    moveHistory = []\n",
    "    indices = []\n",
    "    depth = 0\n",
    "    wins = 0    \n",
    "\n",
    "    for i in range(training_epochs):\n",
    "        while current_state:\n",
    "            move = choose_next_move(deepcopy(current_state), randomness)\n",
    "            current_state.nimming(move)  \n",
    "            reward = eval_state(deepcopy(current_state))\n",
    "            depth += 1\n",
    "            states_history.append((deepcopy(current_state), reward))\n",
    "            \n",
    "            if depth >= max_depth:\n",
    "                break\n",
    "\n",
    "            if current_state:\n",
    "                move = opponent_move(deepcopy(current_state), opponentStrategy=random_strategy)\n",
    "                current_state.nimming(move)\n",
    "                reward = - eval_state(deepcopy(current_state))\n",
    "                states_history.append((deepcopy(current_state), reward))\n",
    "            else:\n",
    "                wins += 1\n",
    "\n",
    "        learn(states_history, randomness, learning_rate)  \n",
    "        if plot_stats and i % 10 == 0:\n",
    "            moveHistory.append(100*wins/(i+1))\n",
    "            indices.append(i)\n",
    "        current_state = deepcopy(initial_state)\n",
    "        depth = 0\n",
    "\n",
    "    if plot_stats:\n",
    "        plt.semilogy(indices, moveHistory, \"b\")\n",
    "\n",
    "    return rl_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversimplified match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "status: Initial board  -> <1 3 5 7 9>\n",
      "Max depth reached: 25, Cache hit ratio: 81.2% (2252237/2772340), Cache entries : 520103\n",
      "status: After player 0 -> <1 3 5 7 0>\n",
      "status: After player 1 -> <0 3 5 7 0>\n",
      "Max depth reached: 25, Cache hit ratio: 81.2% (2254515/2775273), Cache entries : 520758\n",
      "status: After player 0 -> <0 2 5 7 0>\n",
      "status: After player 1 -> <0 2 5 3 0>\n",
      "Max depth reached: 25, Cache hit ratio: 81.2% (2254700/2775525), Cache entries : 520825\n",
      "status: After player 0 -> <0 2 1 3 0>\n",
      "status: After player 1 -> <0 2 1 2 0>\n",
      "Max depth reached: 25, Cache hit ratio: 81.2% (2254725/2775566), Cache entries : 520841\n",
      "status: After player 0 -> <0 2 0 2 0>\n",
      "status: After player 1 -> <0 2 0 0 0>\n",
      "Max depth reached: 25, Cache hit ratio: 81.2% (2254727/2775569), Cache entries : 520842\n",
      "status: After player 0 -> <0 0 0 0 0>\n",
      "status: Player 0 won!\n"
     ]
    }
   ],
   "source": [
    "nim = Nim(5, None)\n",
    "logging.info(f\"status: Initial board  -> {nim}\")\n",
    "player = 0\n",
    "strategy = (minmax(print_stats=True), optimal_strategy)\n",
    "\n",
    "while nim:\n",
    "    ply = strategy[player](nim) \n",
    "    nim.nimming(ply)\n",
    "    logging.info(f\"status: After player {player} -> {nim}\")\n",
    "    player = 1 - player\n",
    "winner = 1 - player\n",
    "logging.info(f\"status: Player {winner} won!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e8c2219f40434aae6d3ece69169492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Played games:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fixed strategy win rate against Optimal strategy was 13.64 % (12.0/88)\n",
      "Fixed strategy win rate against Random strategy was 78.41 % (69.0/88)\n",
      "Evolved strategy win rate against Optimal strategy was 20.45 % (18.0/88)\n",
      "Evolved strategy win rate against Random strategy was 79.55 % (70.0/88)\n",
      "MinMax strategy win rate against Optimal strategy was 15.91 % (14.0/88)\n",
      "MinMax strategy win rate against Random strategy was 77.27 % (68.0/88)\n",
      "RL strategy win rate against Optimal strategy was 12.5 % (11.0/88)\n",
      "RL strategy win rate against Random strategy was 51.14 % (45.0/88)\n"
     ]
    }
   ],
   "source": [
    "games = list((a, b) for a, b in product(range(2, 10), list(range(1, 10)) + [None]) if b == None or b < a)\n",
    "opponents = [('Optimal', optimal_strategy), ('Random', random_strategy)]\n",
    "strategies = [('Fixed', fixed_strategy), ('Evolved', evolvable_strategy([0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0])), ('MinMax', minmax(5)), ('RL', reinforcement_learning)]\n",
    "scores = {a[0]: [0.0 for _ in strategies] for a in opponents}\n",
    "\n",
    "for idx, pars in tqdm(enumerate(games), desc='Played games', total=len(games)):\n",
    "    nim_size, k = pars\n",
    "    \n",
    "    logging.debug(f'Game {idx+1}: Nim({nim_size}, {k})')\n",
    "    \n",
    "    for idx, s in enumerate(strategies):\n",
    "        strategy_name, strategy = s\n",
    "        strategy = strategy if strategy_name !='RL'else strategy(Nim(nim_size, k), max_depth=20, training_epochs=500, randomness=0.2, learning_rate=0.3) # Necessary to train on new type of nim\n",
    "        for opponent_name, opponent in opponents:\n",
    "            strategy = strategy if strategy_name !='MinMax'else minmax(5) # Necessary to reset cache\n",
    "            scores[opponent_name][idx] += evaluate(strategy, opponent, nim_size=nim_size, k=k) \n",
    "            strategy = strategy if strategy_name !='MinMax'else minmax(5) # Necessary to reset cache\n",
    "            scores[opponent_name][idx] += 1 - evaluate(opponent, strategy, nim_size=nim_size, k=k) \n",
    "\n",
    "for idx, strategy in enumerate(strategies):\n",
    "    for opponent_name, _ in opponents:\n",
    "        logging.info(f'{strategy[0]} strategy win rate against {opponent_name} strategy was {round(scores[opponent_name][idx] * 100 / (2 * len(games)), 2)} % ({scores[opponent_name][idx]}/{2 * len(games)})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e660ce8e299eab6e1afd5ba1640493fbea599bc98ebfd90153bb9a99407a2701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
